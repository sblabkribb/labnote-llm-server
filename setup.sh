#!/bin/bash
# Vessl.ai Service í™˜ê²½ì— ìµœì í™”ëœ AI ë°±ì—”ë“œ í™˜ê²½ ì„¤ì • ìŠ¤í¬ë¦½íŠ¸
# ìµœì´ˆ 1íšŒë§Œ ì „ì²´ ì„¤ì¹˜ë¥¼ ìˆ˜í–‰í•˜ê³ , ì´í›„ì—ëŠ” ì˜ì†ì„± ë³¼ë¥¨ì„ í†µí•´ ë¹ ë¥´ê²Œ ì‹œì‘í•©ë‹ˆë‹¤.
set -e

# --- 1. ì˜ì†ì„± ë³¼ë¥¨ ì„¤ì • ë° ì„¤ì¹˜ ì™„ë£Œ í™•ì¸ ---
# Vessl ì„œë¹„ìŠ¤ ì„¤ì •ì˜ Mount Pathì™€ ì¼ì¹˜í•´ì•¼ í•©ë‹ˆë‹¤.
PERSISTENT_DIR="/persistent"
# ì„¤ì¹˜ ì™„ë£Œ ì—¬ë¶€ë¥¼ í™•ì¸í•  í”Œë˜ê·¸ íŒŒì¼ (ë²„ì „ ê´€ë¦¬ë¥¼ ìœ„í•´ v1 ì¶”ê°€)
SETUP_FLAG="${PERSISTENT_DIR}/.setup_complete_v1"

# ì˜ì† ë³¼ë¥¨ì— ëª¨ë¸ê³¼ ë°ì´í„°ë¥¼ ì €ì¥í•  ë””ë ‰í† ë¦¬ ìƒì„±
mkdir -p "${PERSISTENT_DIR}/models/hf"
mkdir -p "${PERSISTENT_DIR}/models/gguf"
mkdir -p "${PERSISTENT_DIR}/ollama_models" # Ollamaê°€ ëª¨ë¸ì„ ì €ì¥í•  ê²½ë¡œ

# í”Œë˜ê·¸ íŒŒì¼ì´ ì¡´ì¬í•˜ë©´, ë¬´ê±°ìš´ ì„¤ì¹˜ ì‘ì—…ì„ ëª¨ë‘ ê±´ë„ˆëœë‹ˆë‹¤.
if [ -f "${SETUP_FLAG}" ]; then
    echo "âœ… Setup has already been completed. Skipping installation and downloads."
else
    echo "ğŸš€ Performing first-time setup... This will take a while."
    
    # --- 2. ì‹œìŠ¤í…œ ë° í•„ìˆ˜ ë„êµ¬ ì„¤ì¹˜ (ìµœì´ˆ 1íšŒ) ---
    echo ">>> (Step 1/5) Updating package lists and installing prerequisites..."
    apt-get update > /dev/null
    apt-get install -y curl gpg > /dev/null
    pip install -q huggingface_hub[cli]
    echo ">>> Prerequisites are up to date."

    # --- 3. Redis Stack ì„œë²„ ì„¤ì¹˜ (ìµœì´ˆ 1íšŒ) ---
    echo ">>> (Step 2/5) Setting up Redis Stack Server..."
    curl -fsSL https://packages.redis.io/gpg | gpg --dearmor -o /usr/share/keyrings/redis-archive-keyring.gpg
    echo "deb [signed-by=/usr/share/keyrings/redis-archive-keyring.gpg] https://packages.redis.io/deb $(lsb_release -cs) main" | tee /etc/apt/sources.list.d/redis.list > /dev/null
    apt-get update > /dev/null
    apt-get install -y redis-stack-server > /dev/null
    echo ">>> Redis installed."
    
    # --- 4. Ollama ì„¤ì¹˜ ë° ëª¨ë“  ëª¨ë¸ ë‹¤ìš´ë¡œë“œ (ìµœì´ˆ 1íšŒ) ---
    echo ">>> (Step 3/5) Setting up Ollama and downloading all models to persistent storage..."
    curl -fsSL https://ollama.com/install.sh | sh
    
    # Ollamaê°€ ëª¨ë¸ì„ ì˜ì† ë³¼ë¥¨ì— ì €ì¥í•˜ë„ë¡ í™˜ê²½ ë³€ìˆ˜ ì„¤ì •
    export OLLAMA_MODELS="${PERSISTENT_DIR}/ollama_models"
    
    # ë°±ê·¸ë¼ìš´ë“œì—ì„œ Ollama ì„ì‹œ ì‹¤í–‰
    ollama serve &
    OLLAMA_PID=$!
    # Ollama ì„œë²„ê°€ ì‹œì‘ë  ì‹œê°„ì„ ì¶©ë¶„íˆ ì¤ë‹ˆë‹¤.
    sleep 10
    
    echo "    - Pulling base models: nomic-embed-text, mixtral, llama3:70b..."
    ollama pull nomic-embed-text > /dev/null
    ollama pull mixtral > /dev/null
    ollama pull llama3:70b > /dev/null
    echo "    - Base models pulled."

    # DPO ê¸°ë³¸ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ (ì˜ì† ë³¼ë¥¨ì—)
    DPO_MODEL_PATH="${PERSISTENT_DIR}/models/hf/Llama3-OpenBioLLM-8B"
    echo "    - Downloading DPO base model to ${DPO_MODEL_PATH}..."
    huggingface-cli download aaditya/Llama3-OpenBioLLM-8B --local-dir "${DPO_MODEL_PATH}" --local-dir-use-symlinks False
    echo "    - DPO base model downloaded."

    # ì¶”ë¡ ìš© GGUF ëª¨ë¸ ë‹¤ìš´ë¡œë“œ (ì˜ì† ë³¼ë¥¨ì—)
    GGUF_MODEL_FILE_PATH="${PERSISTENT_DIR}/models/gguf/llama3-openbiollm-8b.Q4_K_M.gguf"
    echo "    - Downloading Inference GGUF model to ${GGUF_MODEL_FILE_PATH}..."
    huggingface-cli download MoMonir/Llama3-OpenBioLLM-8B-GGUF \
        llama3-openbiollm-8b.Q4_K_M.gguf \
        --local-dir "${PERSISTENT_DIR}/models/gguf" --local-dir-use-symlinks False
    echo "    - GGUF model downloaded."
    
    # ì„ì‹œ Ollama ì¢…ë£Œ
    kill $OLLAMA_PID
    sleep 5
    
    # --- 5. ì„¤ì¹˜ ì™„ë£Œ í”Œë˜ê·¸ ìƒì„± ---
    echo ">>> (Step 4/5) First-time setup complete. Creating flag file."
    touch "${SETUP_FLAG}"
fi

# --- 6. ì„œë¹„ìŠ¤ ì‹œì‘ (ë§¤ë²ˆ ì‹¤í–‰) ---
echo ">>> (Step 5/5) Starting core services..."

# Redisê°€ ì‹¤í–‰ ì¤‘ì´ ì•„ë‹ˆë©´ ë°±ê·¸ë¼ìš´ë“œì—ì„œ ì‹¤í–‰
if ! pgrep -f redis-stack-server > /dev/null; then
    redis-stack-server --daemonize yes
    echo ">>> Redis Stack Server started."
else
    echo ">>> Redis is already running."
fi

# Ollamaê°€ ì‹¤í–‰ ì¤‘ì´ ì•„ë‹ˆë©´ ë°±ê·¸ë¼ìš´ë“œì—ì„œ ì‹¤í–‰ (ì˜ì† ë³¼ë¥¨ ê²½ë¡œ ì‚¬ìš©)
if ! pgrep -f "ollama serve" > /dev/null; then
    export OLLAMA_MODELS="${PERSISTENT_DIR}/ollama_models"
    ollama serve &
    sleep 5 # ì„œë²„ ì´ˆê¸°í™” ì‹œê°„
    echo ">>> Ollama server started from persistent storage."
else
    echo ">>> Ollama is already running."
fi

# --- 7. ì¶”ë¡  ëª¨ë¸ Ollamaì— ë“±ë¡ (ë§¤ë²ˆ í™•ì¸ í›„ í•„ìš”ì‹œ ì‹¤í–‰) ---
INFERENCE_MODEL_NAME="biollama3"
GGUF_MODEL_FILE_PATH="${PERSISTENT_DIR}/models/gguf/llama3-openbiollm-8b.Q4_K_M.gguf"
# 'ollama list'ì— ëª¨ë¸ ì´ë¦„ì´ ì—†ëŠ” ê²½ìš°ì—ë§Œ ìƒˆë¡œ ìƒì„±
if ! ollama list | grep -q "${INFERENCE_MODEL_NAME}"; then
    echo "    - Inference model '${INFERENCE_MODEL_NAME}' not found in Ollama. Registering from GGUF..."
    cat <<EOF > /tmp/Modelfile
FROM ${GGUF_MODEL_FILE_PATH}
TEMPLATE """<|begin_of_text|><|start_header_id|>system<|end_header_id|>

{{ .System }}<|eot_id|><|start_header_id|>user<|end_header_id|>

{{ .Prompt }}<|eot_id|><|start_header_id|>assistant<|end_header_id|>
"""
PARAMETER stop "<|eot_id|>"
PARAMETER stop "<|end_of_text|>"
EOF
    ollama create "${INFERENCE_MODEL_NAME}" -f /tmp/Modelfile
    echo ">>> Inference LLM '${INFERENCE_MODEL_NAME}' registered."
else
    echo ">>> Inference LLM '${INFERENCE_MODEL_NAME}' already exists."
fi

# --- 8. Python ì˜ì¡´ì„± ì„¤ì¹˜ ë° .env íŒŒì¼ ìƒì„± ---
echo ">>> Setting up backend environment..."
# Vessl ì‹¤í–‰ ëª…ë ¹ì–´ì˜ cd ëª…ë ¹ì–´ë¥¼ ê³ ë ¤í•˜ì—¬ ê²½ë¡œë¥¼ ê³ ì •í•©ë‹ˆë‹¤.
BACKEND_DIR="/root/labnote-llm-server/labnote-ai-backend"

# .env íŒŒì¼ì´ ì—†ìœ¼ë©´ ìƒì„±
if [ ! -f "${BACKEND_DIR}/.env" ]; then
    echo "    - Creating .env file..."
    cat << EOF > "${BACKEND_DIR}/.env"
# Backend Server Configuration
REDIS_URL="redis://localhost:6379/0"
OLLAMA_BASE_URL="http://127.0.0.1:11434"

# Model Configuration
EMBEDDING_MODEL="nomic-embed-text"
LLM_MODEL="${INFERENCE_MODEL_NAME}"

# DPO Training Configuration
BASE_MODEL_PATH="${PERSISTENT_DIR}/models/hf/Llama3-OpenBioLLM-8B"
NEW_MODEL_NAME="biollama3-v2-dpo"

# DPO Git Repository Configuration (í† í°ì€ Vessl Secret ì‚¬ìš© ê¶Œì¥)
DPO_TRAINER_REPO_URL="https://github.com/sblabkribb/labnote-dpo-trainer.git"
DPO_REPO_LOCAL_PATH="${BACKEND_DIR}/labnote-dpo-trainer-data"
GIT_AUTH_TOKEN="YOUR_GITHUB_TOKEN"
EOF
fi

# ì˜ì¡´ì„± íŒ¨í‚¤ì§€ëŠ” ë§¤ë²ˆ ë¹ ë¥´ê²Œ í™•ì¸/ì„¤ì¹˜í•©ë‹ˆë‹¤.
pip install -r "${BACKEND_DIR}/requirements.txt" > /dev/null
echo ">>> Python dependencies are up to date."
echo "--- Setup script finished. The application is ready to start. ---"